	# Конспект
Регуляризация Моро-Йошиды "сглаживает" исходную функцию $f$, добавляя квадратичный штраф за отклонение от точки s. В основном применяется к негладким и выпуклым функциям.
Формула:![[Pasted image 20250417150502.png]]$f\left(x\right)$ - произвольная функция. $f:R^{d}\to R\cup\left\lbrace\infty\right\rbrace$, при $\lambda>0$.
$\frac{\Vert s-x\Vert^2}{2\lambda}$ - квадратичное ядро(квадратичный штраф)
- s - фиксированная точка
- x - переменная, которую мы оптимизируем
- λ>0 - параметр, регулирующий силу штрафа
inf - Поиск точки y, где сумма $\frac{\Vert s-x\Vert^2}{2\lambda}+f\left(x\right)$ минимальна.
- Чем больше $\lambda$, тем сильнее "размытие" (функция становится более гладкой).
- Чем меньше $\lambda$, тем ближе $f_{\lambda}\left(y\right)$ к исходной $f\left(x\right)$.
Формула проксимального оператора:![[Pasted image 20250417223036.png]]Что он делает?
- Находит компромисс между минимизацией $f(x)$ и близостью к s.
- Делает негладкие функции "удобными" для оптимизации.
Ну по сути проксимальный оператор можно назвать аналогом Моро-Йошиды.
Преобразование Лежандра-Фенхеля:![[Pasted image 20250417224620.png]]Огибающая Моро может быть выражена через преобразование.
**Алгоритмы вычисления огибающей Моро**:
1. Linear-time Legendre Transform(LLT). Использует выпуклость и преобразование Лежандра-Фенхеля. Работает за линейное время $O\left(n+m\right)$ для одномерных функций.
2. Parabolic Envelope(PE). Основан на вычислении нижней огибающей парабол. Также имеет линейную сложность $O\left(n+m\right)$.
3. NonExpansive Proximal mapping(NEP). Использует нерасширяемость проксимального отображения для выпуклых функций. Работает за линейное время $O\left(n+m\right)$.

**Регуляризация Моро-Йосиды**: Для функции $f:R^{n}\to R\cup\left\lbrace{+\infty}\right\rbrace$ регуляризация определяется как:![[Pasted image 20250417230401.png]],где M - симметричная положительная определённая матрица.
Регуляризация преобразует исходную негладкую задачу в гладкую, сохраняя минимумы.
*Основные свойства:* 
- η(x) является выпуклой и непрерывно дифференцируемой.
- Градиент $g(x)=∇η(x)=M(x−p(x))$, где p(x) — проксимальная точка.
- Градиент g(x) липшицев, но может быть негладким.
# 1-й файл
Регуляризация Моро-Йосиды "сглаживает" исходную функцию f, добавляя квадратичный штраф за отклонение от точки s. В основном применяется к негладким и выпуклым функциям.
Формула:![[Pasted image 20250417150502.png]]$f\left(x\right)$ - произвольная функция. $f:R^{d}\to R\cup\left\lbrace\infty\right\rbrace$, при $\lambda>0$.
$\frac{\Vert s-x\Vert^2}{2\lambda}$ - квадратичное ядро(квадратичный штраф)
inf - Поиск точки y, где сумма $\frac{\Vert s-x\Vert^2}{2\lambda}+f\left(x\right)$ минимальна.
- Чем больше $\lambda$, тем сильнее "размытие" (функция становится более гладкой).
- Чем меньше $\lambda$, тем ближе $f_{\lambda}\left(y\right)$ к исходной $f\left(x\right)$.
Формула проксимального оператора:![[Pasted image 20250417223036.png]]Эта формула — проксимальный оператор, который:
- Находит компромисс между минимизацией f(x) и близостью к s.
- Делает негладкие функции "удобными" для оптимизации.
Ну по сути проксимальный оператор можно назвать аналогом Моро-Йосиды.
Преобразование Лежандра-Фенхеля:![[Pasted image 20250417224620.png]]Огибающая Моро может быть выражена через преобразование.
**Алгоритмы вычисления огибающей Моро**:
1. Linear-time Legendre Transform(LLT). Использует выпуклость и преобразование Лежандра-Фенхеля. Работает за линейное время $O\left(n+m\right)$ для одномерных функций.
2. Parabolic Envelope(PE). Основан на вычислении нижней огибающей парабол. Также имеет линейную сложность $O\left(n+m\right)$.
3. NonExpansive Proximal mapping(NEP). Использует нерасширяемость проксимального отображения для выпуклых функций. Работает за линейное время $O\left(n+m\right)$.
### 2-й файл
Изучение свойств регуляризации Моро-Ёсиды для задач выпуклой оптимизации с негладкими ограничениями, в частности, вторых производных регуляризованной функции.
**Регуляризация Моро-Ёсиды:**  
Для негладкой выпуклой функции ff регуляризация определяется как:
![[Pasted image 20250417160307.png]]
где $M$ — положительно определённая матрица.
- Градиент $η(x)$ липшицев и задаётся как $g(x)=M(x−p(x))$, где $p(x)$ — проксимальная точка.
### 3-й файл
Для выпуклой функции ff огибающая Моро-Йосиды определяется как:
![[Pasted image 20250417152107.png]]
где λ>0. Эта функция является дифференцируемой и выпуклой.
Производная FλFλ​ выражается через проксимальное отображение $p_{\lambda}(x)$:
![[Pasted image 20250417152239.png]]
где $∂f$ - субдифференциал функции $f$.
**Гипотеза о кусочно-гладкости:**
- Если $f$ — максимум линейных функций ($f(x)=max\left\lbrace{f_{i}(x):i\in J}\right\rbrace$), то $G_{\lambda}$ кусочно-аффинна (и, следовательно, полугладкая).
- Для случая, когда $f_{i}$ — дважды непрерывно дифференцируемые выпуклые функции, авторы доказывают, что $G_{\lambda}$​ кусочно-гладкая при выполнении условия _Constant Rank Constraint Qualification (CRCQ)_.
*Условие CRCQ:*
CRCQ выполняется в точке $p_{\lambda}\left(x\right)$, если для любого подмножества $K\subseteq J(p_{\lambda}(x))$ ранг семейства векторов ![[Pasted image 20250417152845.png]]
постоянен в окрестности $p_{\lambda}\left(x\right)$.
**Теоремы:**
1. Теорема 1: Если $f_{i}$​ — дважды непрерывно дифференцируемые выпуклые функции и CRCQ выполняется в $p_{\lambda}(x)$, то $G_{\lambda}$​ кусочно-гладкая в окрестности x.
2. Теорема 2: Если $f_{i}$ — линейные функции, то $G_{\lambda}$​ кусочно-аффинна и полугладкая.

*Ключевые термины*:
1. Огибающая Моро-Йосиды
2. Проксимальное отображение
3. Субдифференциал 
4. Кусочно-гладкая функция 
5. Полугладкая функция
6. Условие постоянного ранга


# 2-й файл
Изучение свойств регуляризации Моро-Йосиды для задач выпуклой оптимизации с негладкими целевыми функциями и гладкими ограничениями.
**Регуляризация Моро-Йосиды**: Для функции $f:R^{n}\to R\cup\left\lbrace{+\infty}\right\rbrace$ регуляризация определяется как:![[Pasted image 20250417230401.png]],где M - симметричная положительная определённая матрица.
Регуляризация преобразует исходную негладкую задачу в гладкую, сохраняя минимумы.
*Основные свойства:* 
- η(x) является выпуклой и непрерывно дифференцируемой.
- Градиент $g(x)=∇η(x)=M(x−p(x))$, где p(x) — проксимальная точка.
- Градиент g(x) липшицев, но может быть негладким.
Статья углубляет теорию регуляризации Моро-Йосиды, показывая, что градиент регуляризованной функции обладает свойствами, необходимыми для эффективной оптимизации. Это открывает путь для применения мощных численных методов к негладким задачам с ограничениями.
# 3-й файл
Статья подтверждает гипотезу о кусочной гладкости производной огибающей Моро-Йосиды для широкого класса негладких выпуклых функций. Это важно для разработки эффективных численных методов, таких как обобщённый метод Ньютона, в негладкой оптимизации. Результаты показывают, что даже для сложных негладких функций регуляризация Моро-Йосиды сохраняет полезные аналитические свойства
# Ключевые термины
Функция называется **негладкой**, если она не является дифференцируемой (хотя бы один раз) в некоторых точках своей области определения. Это означает, что её производная либо не существует, либо имеет разрывы.
**Выпуклая** функция это если соединить две точки на графике прямой, то график функции лежит под этой прямой.
**Сглаженная версия Моро-Йошиды**![[Pasted image 20250417150502.png]]$f\left(x\right)$ - произвольная функция. $f:R^{d}\to R\cup\left\lbrace\infty\right\rbrace$, при $\lambda>0$.
$\frac{\Vert s-x\Vert^2}{2\lambda}$ - квадратичное ядро(квадратичный штраф)
- s - фиксированная точка
- x - переменная, которую мы оптимизируем
- λ>0 - параметр, регулирующий силу штрафа
inf - Поиск точки y, где сумма $\frac{\Vert s-x\Vert^2}{2\lambda}+f\left(x\right)$ минимальна.
- Чем больше $\lambda$, тем сильнее "размытие" (функция становится более гладкой).
- Чем меньше $\lambda$, тем ближе $f_{\lambda}\left(y\right)$ к исходной $f\left(x\right)$.
![[Pasted image 20250417183802.png]]
Чем больше λ, тем $M_{\lambda}\left(s\right)$ глаже. (Но за это приходится платить меньшей точностью приближения к f(x))
**Проксимальный оператор:** ![[Pasted image 20250417172458.png]]$\operatorname*{\mathrm{arg~min}}_{x\in R^{d}}$ - находит точку x, которая оптимально балансирует между приближением к s и минимизацией $f(x)$.
**Преобразование Лежандра-Фенхеля**![[Pasted image 20250417224620.png]]$g_{\lambda}^{\cdot}$ - сопряжённая функция.
Если сложно напрямую минимизировать $\frac{\Vert s-x\Vert^2}{2\lambda}+f\left(x\right)$, можно сначала найти $g_{\lambda}^{\cdot}$, а затем получить $M_{\lambda}$.
